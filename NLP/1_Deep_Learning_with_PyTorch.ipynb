{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example : logistic regression bag-of-words classifier \n",
    "\n",
    "Read it from here \n",
    "https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html\n",
    "\n",
    "The BoW vector for the sentence “hello hello hello hello”\n",
    "> hello -- index 0 -- bow vector [4, 0] \n",
    "\n",
    "For “hello world world hello”, it is\n",
    "> world -- index 1 -- bow vector [2, 2]\n",
    "\n",
    "The above function as be represented as:\n",
    "> [count(hello), count(world)]\n",
    "\n",
    "this vector is x\n",
    "output of the network is log softmax(Ax + b)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: [(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH'), (['Give', 'it', 'to', 'me'], 'ENGLISH'), (['No', 'creo', 'que', 'sea', 'una', 'buena', 'idea'], 'SPANISH'), (['No', 'it', 'is', 'not', 'a', 'good', 'idea', 'to', 'get', 'lost', 'at', 'sea'], 'ENGLISH')]\n",
      "Test Data: [(['Yo', 'creo', 'que', 'si'], 'SPANISH'), (['it', 'is', 'lost', 'on', 'me'], 'ENGLISH')]\n"
     ]
    }
   ],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "\n",
    "print(\"Data: {}\".format(data))\n",
    "print(\"Test Data: {}\".format(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 16, 'on': 25, 'at': 22, 'not': 17, 'que': 11, 'a': 18, 'la': 4, 'si': 24, 'en': 3, 'creo': 10, 'No': 9, 'lost': 21, 'to': 8, 'buena': 14, 'cafeteria': 5, 'Give': 6, 'comer': 2, 'it': 7, 'sea': 12, 'get': 20, 'Yo': 23, 'me': 0, 'gusta': 1, 'good': 19, 'idea': 15, 'una': 13}\n"
     ]
    }
   ],
   "source": [
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    # the first index is basically the collection of words \n",
    "    # the second index is the language \n",
    "    #print(sent)\n",
    "    for word in sent:\n",
    "        #print(word)\n",
    "        if word not in word_to_ix:\n",
    "            # the item key is the word itself\n",
    "            # the number to the key is a unique no\n",
    "            # here we are just saying the index is the next one \n",
    "            # of the previous highes index\n",
    "            # suppose already words in dict is 2 then index = 0, 1 \n",
    "            # so when the third word comes we put it as index 2\n",
    "            # i.e. len(word_to_ix)\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "print(word_to_ix)            \n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "\n",
    "#here basically they are mapping words to languages \n",
    "#words are expressed in terms of vocabulary\n",
    "#and there are two languages so NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  \n",
    "        # Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  \n",
    "        #In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not \n",
    "        #have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions \n",
    "        # are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    \"\"\"\n",
    "    the program receives a sentence which is a list of words \n",
    "    whose vec representation it wants as output\n",
    "    \n",
    "    word_to_ix is which word means which index ?\n",
    "    so we write a loop which goes over all the individual words in \n",
    "    the sentence and retrieves the index of the particular word as \n",
    "    \n",
    "    word_to_ix[word]\n",
    "    \n",
    "    then whatever index is retrieved the count is increased \n",
    "    so a sentence will have a representation of voc length\n",
    "    with each position signifying the count of words occurance \n",
    "    the position is determined by the index stored in word_to_ix\n",
    "    \n",
    "    \"\"\"\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    \"\"\"\n",
    "    a same function as above \n",
    "    this returns the index of the labels \n",
    "    so label_to_ix is like \n",
    "    '0' : english , '1' : spanish \n",
    "    \n",
    "    label is the language whose index we desire\n",
    "    \"\"\"\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoWClassifier(\n",
      "  (linear): Linear(in_features=26, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('linear.weight', Parameter containing:\n",
      "tensor([[ 0.1953,  0.1572, -0.0092, -0.1309,  0.1194,  0.0609, -0.1268,\n",
      "          0.1274,  0.1191,  0.1739, -0.1099, -0.0323, -0.0038,  0.0286,\n",
      "         -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,  0.0644,\n",
      "          0.0431,  0.0713,  0.0972, -0.1816,  0.0987],\n",
      "        [-0.1379, -0.1480,  0.0119, -0.0334,  0.1152, -0.1136, -0.1743,\n",
      "          0.1427, -0.0291,  0.1103,  0.0630, -0.1471,  0.0394,  0.0471,\n",
      "         -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,  0.1796,\n",
      "         -0.0363,  0.1106,  0.0849, -0.1268, -0.1668]]))\n",
      "('linear.bias', Parameter containing:\n",
      "tensor([ 0.1882,  0.0102]))\n",
      "torch.Size([2, 26])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# the model knows its parameters.  \n",
    "#The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable \n",
    "#in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge \n",
    "#of the nn.Linear's parameters\n",
    "for param in model.named_parameters():\n",
    "    print(param)\n",
    "for param in model.parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Give', 'it', 'to', 'me'], 'ENGLISH')\n",
      "['Give', 'it', 'to', 'me'] ENGLISH\n",
      "torch.Size([1, 26])\n",
      "tensor([[-0.4060, -1.0976]])\n"
     ]
    }
   ],
   "source": [
    "# To run the model, pass in a BoW vector\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    sample = data[1]\n",
    "    \n",
    "    print(sample)\n",
    "    \n",
    "    print(sample[0], sample[1])\n",
    "    \n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "    print(bow_vector.shape)\n",
    "    \n",
    "    log_probs = model(bow_vector)\n",
    "    \n",
    "    print(log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Which of the above values corresponds to the log probability \n",
    "of ENGLISH, and which to SPANISH? We never defined it, but \n",
    "we need to if we want to train the thing.\n",
    "\"\"\"\n",
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yo', 'creo', 'que', 'si']\n",
      "Log probs before train: tensor([[-0.6553, -0.7325]])\n",
      "Probs before train: tensor([[ 0.5193,  0.4807]])\n",
      "['it', 'is', 'lost', 'on', 'me']\n",
      "Log probs before train: tensor([[-0.3468, -1.2274]])\n",
      "Probs before train: tensor([[ 0.7070,  0.2930]])\n",
      "tensor([-0.1099,  0.0630])\n",
      "*****Training******\n",
      "['Yo', 'creo', 'que', 'si']\n",
      "Log probs after train: tensor([[-0.3411, -1.2413]])\n",
      "Probs after test: tensor([[ 0.7110,  0.2890]])\n",
      "['it', 'is', 'lost', 'on', 'me']\n",
      "Log probs after train: tensor([[-1.2239, -0.3483]])\n",
      "Probs after test: tensor([[ 0.2941,  0.7059]])\n",
      "tensor([ 0.1145, -0.1613])\n"
     ]
    }
   ],
   "source": [
    "# Run on test data before we train, \n",
    "# just to see a before-and-after\n",
    "import numpy as np\n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(instance)\n",
    "        print(\"Log probs before train: {}\".format(log_probs))\n",
    "        print(\"Probs before train: {}\".format(np.exp(log_probs)))\n",
    "        \n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "\n",
    "# as we have used the log softmax we use NLL loss\n",
    "# had we not used the log softmax we would have used \n",
    "# the cross-entropy loss (which does log softmax)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"*****Training******\")\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets \n",
    "# have more than two instances.  \n",
    "# Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    \n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also \n",
    "        # we must wrap the target in a Tensor \n",
    "        # as an integer. For example, if the \n",
    "        # target is SPANISH, then we wrap the \n",
    "        # integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        \n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, \n",
    "        # and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(log_probs, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()     \n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(instance)\n",
    "        print(\"Log probs after train: {}\".format(log_probs))\n",
    "        print(\"Probs after test: {}\".format(np.exp(log_probs)))\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
